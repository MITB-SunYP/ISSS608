---
title: "Visual Statistical Analysis"
author: "Sun Yiping"
date: "January 31, 2024"
date-modified: "Febrary 2, 2024"
execute: 
  warning: false
---

## 1. Learning Outcome

In this hands-on exercise, we will learn the following:

-   *ggstatsplot* package to create visual graphics with rich statistical information,

-   performance package to visualise model diagnostics, and

-   parameters package to visualise model parameters

## 2. Getting Started

### 2.1 Installing and loading the required libraries

Firstly, let's install and load the required packages:

-   [tidyverse](https://www.tidyverse.org/): an opinionated collection of R packages designed for data import, data wrangling and data exploration

-   [ggstatsplot](https://cran.r-project.org/web/packages/ggstatsplot/index.html): an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves

```{r}
pacman::p_load(tidyverse, ggstatsplot)

```

### 2.2 Importing the data

Similar to the previous hands-on exercises, we'll still use Exam_data for this exercise. The data file contains year end examination grades of a cohort of primary 3 students from a local school, and it's in csv format.

Let's start by importing the data.

```{r}
exam <- read_csv("../../Data/Exam_data.csv")

```

## 3. One-sample test: gghistostats() method

In the code chunk below, *gghistostats()* is used to to build an visual of one-sample test on English scores.

```{r}
set.seed(1234)

gghistostats(
  data = exam,
  x = ENGLISH,
  type = "bayes",
  test.value = 60,
  xlab = "English scores"
)

```
Default information: - statistical details - Bayes Factor - sample sizes - distribution summary

### 3.1 Unpacking the Bayes Factor

-   A Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.

-   That’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.

-   When we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.

-   The [Schwarz criterion](https://www.statisticshowto.com/bayesian-information-criterion/) is one of the easiest ways to calculate rough approximation of the Bayes Factor.

### 3.2 How to interpret Bayes Factor

A Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by [Lee and Wagenmakers](https://www-tandfonline-com.libproxy.smu.edu.sg/doi/pdf/10.1080/00031305.1999.10474443?needAccess=true) in 2013.

## 4. Two-sample mean test: ggbetweenstats()

In the code chunk below, *ggbetweenstats()* is used to build a visual for two-sample mean test of Maths scores by gender.

```{r}
ggbetweenstats(
  data = exam,
  x = GENDER, 
  y = MATHS,
  type = "np",
  messages = FALSE
)

```

Default information: - statistical details - Bayes Factor - sample sizes - distribution summary

::: callout-tip

## Try it out

The Mann-Whitney statistic is displayed on the plot to tell if the two distributions are significantly different. Since the p-value is larger than 0.05, we can conclude that the maths scores between female students and male students are not significantly different at 5% significance level.

:::

## 5. Oneway ANOVA Test: ggbetweenstats() method

In the code chunk below, *ggbetweenstats()* is used to build a visual for One-way ANOVA test on English score by race.

```{r}
ggbetweenstats(
  data = exam,
  x = RACE, 
  y = ENGLISH,
  type = "p",
  mean.ci = TRUE, 
  pairwise.comparisons = TRUE, 
  pairwise.display = "s",
  p.adjust.method = "fdr",
  messages = FALSE
)

```

A few values that we can choose for pairwise.display:
-   “ns” → only non-significant
-   “s” → only significant
-   “all” → everything

::: callout-tip

## Try it out

Welch test is performed to check if the English scores varies among different races significantly. Since the p-value is less than 0.05, we can conclude that the English scores distributed significantly different among races at 5% significance level.

:::

## 6. Significant Test of Correlation: ggscatterstats()

In the code chunk below, *ggscatterstats()* is used to build a visual for Significant Test of Correlation between Maths scores and English scores.

```{r}
ggscatterstats(
  data = exam,
  x = MATHS,
  y = ENGLISH,
  marginal = FALSE,
  )

```

::: callout-tip

## Try it out

The plot above shows that there is a strong positive correlation between English scores and Maths scores, as the Pearson correlation coefficient is 0.83.

:::

## 7. Significant Test of Association (Depedence) : ggbarstats() methods

In the code chunk below, the Maths scores is binned into a 4-class variable by using *cut()*.

```{r}
exam1 <- exam %>% 
  mutate(MATHS_bins = 
           cut(MATHS, 
               breaks = c(0,60,75,85,100))
)

```

In this code chunk below *ggbarstats()* is used to build a visual for Significant Test of Association.

```{r}
ggbarstats(exam1, 
           x = MATHS_bins, 
           y = GENDER)

```

## 8. Visualising Models

In this section, we will learn how to visualise model diagnostic and model parameters by using *parameters* package.

-   Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.

### 8.1 Installing and loading the required libraries

In this exercise, ggstatsplot and tidyverse will be used.

```{r}
pacman::p_load(readxl, performance, parameters, see, ggstatsplot, tidyverse)

```

### 8.2 Importing Excel file: readxl methods

Let's first import the data using *read_xls()* function from **readxl** package. In this exercise, we'll use the *data* worksheet of *ToyotaCorolla.xls* workbook.

```{r}
car_resale <- read_xls("../../Data/ToyotaCorolla.xls", "data")

car_resale

```

::: callout-tip
## Try it out

The data is imported as a tibble table in R. There are 1,436 rows, and 38 columns in the dataset.

-   5 categorical variables
-   33 numerical variables
:::

### 8.3 Multiple Regression Model using lm()

The code chunk below is used to calibrate a **multiple linear regression model** by using *lm()* of Base Stats of R.

```{r}
model <- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, 
            data = car_resale)

model

```

::: callout-tip
## Try it out

The results above provides the intercept of the model and the coefficients of the factors.

-   intercept: -2.637e+06
-   Age_08_04: -1.409e+01
-   Mfg_Year: 1.315e+03
-   KM: -2.323e-02
-   Weight: 1.903e+01
-   Guarantee_Period: 2.770e+01
:::

### 8.4 Model Diagnostic: checking for multicolinearity:

In the code chunk, *check_collinearity()* of **performance** package is used to check if multicolinearity exists in the model.

```{r}
check_collinearity(model)

```

::: callout-tip
## Try it out

The results above shows that Age_08_04 and Mfg_Year is highly correlated as the VIF value is larger than 10.

In this case, we shall remove one of them and retrain the model.
:::

```{r}
check_c <- check_collinearity(model)
plot(check_c)

```

### 8.5 Model Diagnostic: checking normality assumption

Let's remove the manufacturing year and retrain the model.

```{r}
model1 <- lm(Price ~ Age_08_04 + KM + Weight + Guarantee_Period, 
             data = car_resale)

```

In the code chunk, *check_normality()* of **performance** package is used to check if the residuals of the model follows the normal distribution.

```{r}
check_n <- check_normality(model1)

```

```{r}
plot(check_n)

```

::: callout-tip
## Try it out

The plot above shows that some residuals are not along the 0 reference line, which indicates that the residuals are not normally distributed.
:::

### 8.6 Model Diagnostic: Check model for homogeneity of variances

In the code chunk, *check_heteroscedasticity()* of **performance** package is used to check for homogeneity of variance.

```{r}
check_h <- check_heteroscedasticity(model1)

```

```{r}
plot(check_h)

```

::: callout-tip
## Try it out

As the reference line shows an upward trend, it indicates that heterogeneity exists in the residuals.
:::

### 8.7 Model Diagnostic: Complete check

We can also perform the complete checkings by using *check_model()* function.

```{r fig.height=10}
check_model(model1)

```

### 8.8 Visualising Regression Parameters: see methods

In the code below, *plot()* of **see** package and *parameters()* of **parameters** package is used to visualise the parameters of a regression model.

```{r}
plot(parameters(model1))

```

::: callout-tip
## Try it out

It's clear from the plot above that Age_08_04 and KM have negative impact on the price, whereas weight and guarantee period have positive impact on the price.

A factor with a negative impact indicates that the higher the value is, the lower the price is. On the other hand, a factor with a postive impact indicates that the higher the values is, the higher the price is.
:::

### 8.9 Visualising Regression Parameters: ggcoefstats() methods

In the code below, *ggcoefstats()* of **ggstatsplot** package to visualise the parameters of a regression model.

```{r}
ggcoefstats(model1, 
            output = "plot")

```



This comes to the end of this hands-on exercise. Hope you enjoyed it, too!

See you in the next hands-on exercise 🥰





